{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "import numpy as np\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from collections import defaultdict\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = {'datafile': '../data_cleaning/articles1.csv',\n",
    "'text_label': 'content',\n",
    "'y_label': 'label',\n",
    "'test_data': 'test',\n",
    "'fts_to_try': ['TfidfVectorize'\n",
    "              ,'CountPOS']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_params():\n",
    "    \n",
    "    params = {'CountVectorize': {\n",
    "                 'ngram_range':[(1,1)\n",
    "#                                 , (1,2)\n",
    "                               ]},\n",
    "              'TfidfVectorize': {\n",
    "                 'ngram_range':[(1,2)]},\n",
    "              'CountPOS':{\n",
    "                 'language': ['english']}}\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureGenerator():\n",
    "    '''\n",
    "    Generates a set of features given a labeled dataset.\n",
    "    Creates a reusable pipeline to generate the same features for future unknown examples.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, datafile, text_label, y_label, fts_to_try):\n",
    "        self.text_label = text_label\n",
    "        self.y_label = y_label\n",
    "        self.fts_to_try = fts_to_try\n",
    "        \n",
    "        # get features and parameters to try\n",
    "        self.params = define_params()\n",
    "        \n",
    "        # Read in data\n",
    "        self.data = pd.read_csv(datafile)\n",
    "        self.raw_text = self.data[text_label]\n",
    "        self.y = self.data[y_label]\n",
    "        self.pipeline = {}\n",
    "        \n",
    "        # Generate features from raw text\n",
    "        self.X = self.fit()\n",
    "        \n",
    "    def transform(self, new_datafile):\n",
    "        '''\n",
    "        Generate features for never before seen data.\n",
    "        '''\n",
    "        self.new_data = pd.read_csv(new_datafile)\n",
    "        self.new_raw_text = self.new_data[self.text_label]\n",
    "        \n",
    "        X = None\n",
    "        \n",
    "        for step in self.pipeline.keys():\n",
    "            feat_generator = getattr(self, step)\n",
    "            x_features = feat_generator(step=step)\n",
    "            if X != None:\n",
    "                    X = hstack((X, x_features))     \n",
    "            else:\n",
    "                X = x_features\n",
    "        \n",
    "        self.new_X = X\n",
    "        print(\"{} features generated for {} examples\".format((self.new_X.shape)[1], (self.new_X.shape)[0]))\n",
    "            \n",
    "        \n",
    "    def fit(self):\n",
    "        '''\n",
    "        Generates features for labeled data.\n",
    "        Saves feature generator objects (e.g. fitted vectorizers)\n",
    "        for future use with unlabeled data.\n",
    "        '''\n",
    "        X = None\n",
    "        for f in self.fts_to_try:\n",
    "            print(\"Creating feature: \",f)\n",
    "            parameter_values = self.params[f]\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                print(\"Paramaters: \",p)\n",
    "                feat_generator = getattr(self, f)\n",
    "                x_features, transformer = feat_generator(p)\n",
    "                self.pipeline[f] = transformer\n",
    "                if X != None:\n",
    "                    X = hstack((X, x_features))     \n",
    "                else:\n",
    "                    X = x_features\n",
    "        print(\"{} features generated for {} examples\".format((X.shape)[1], (X.shape)[0]))\n",
    "        return X\n",
    "    \n",
    "    ### BEGIN FEATURE GEN FUNCTIONS ###\n",
    "    ### Any of these function names can be added to \n",
    "    ### the list of features to try in feature_config.yaml.\n",
    "    ### Additional parameters combinations can be defined in define_params()\n",
    "\n",
    "    \n",
    "    def CountVectorize(self, kwargs=None, step='fit'):\n",
    "        '''\n",
    "        Creates a sparse matrix of normalized counts of words from document.\n",
    "        kwargs are generated from the paramaters dictionary\n",
    "        '''\n",
    "        if step == 'fit':\n",
    "            v = CountVectorizer(tokenizer=nltk.word_tokenize,\n",
    "                             stop_words='english',\n",
    "                             max_features=3000, **kwargs)\n",
    "            x_features = v.fit_transform(self.raw_text, self.y)\n",
    "            print(\"xft size\", x_features.shape)\n",
    "            return x_features, v\n",
    "        else:\n",
    "            v = self.pipeline[step]\n",
    "            x_features = v.transform(self.new_raw_text)\n",
    "            return x_features\n",
    "\n",
    "    \n",
    "    def TfidfVectorize(self, kwargs=None, step='fit'):\n",
    "        '''\n",
    "        Creates a sparse matrix of normalized TFIDF counts of words from document.\n",
    "        kwargs are generated from the paramaters dictionary\n",
    "        '''\n",
    "        if step == 'fit':\n",
    "            v = TfidfVectorizer(tokenizer=nltk.word_tokenize,\n",
    "                             stop_words='english',\n",
    "                             max_features=3000, **kwargs)\n",
    "            x_features = v.fit_transform(self.raw_text, self.y)\n",
    "            return x_features, v\n",
    "        else:\n",
    "            v = self.pipeline[step]\n",
    "            x_features = v.transform(self.new_raw_text)\n",
    "            return x_features\n",
    "    \n",
    "    def CountPOS(self, kwargs=None, step='fit'):\n",
    "        '''\n",
    "        Creates a sparse matrix of part of speech frequencies for each document.\n",
    "        kwargs are generated from the paramaters dictionary\n",
    "        '''\n",
    "        def count_pos(s):\n",
    "            tagged = pos_tag(word_tokenize(s))\n",
    "            counts = defaultdict(int)\n",
    "            for (word, tag) in tagged:\n",
    "                counts[tag] += 1\n",
    "            for k, v in counts.items():\n",
    "                counts[k] = counts[k]/sum(counts.values())\n",
    "            return counts\n",
    "\n",
    "        if step == 'fit':\n",
    "            v = DictVectorizer()\n",
    "            d = self.raw_text.apply(count_pos)\n",
    "            x_features = v.fit_transform(d)\n",
    "            return x_features, v\n",
    "        else:\n",
    "            v = self.pipeline[step]\n",
    "            d = self.new_raw_text.apply(count_pos)\n",
    "            x_features = v.transform(d)\n",
    "            return x_features\n",
    "        \n",
    "        \n",
    "    ### END FEATURE GEN FUNCTIONS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature:  TfidfVectorize\n",
      "Paramaters:  {'ngram_range': (1, 2)}\n",
      "Creating feature:  CountPOS\n",
      "Paramaters:  {'language': 'english'}\n",
      "3041 features generated for 10 examples\n",
      "3041 features generated for 5 examples\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(\"feature_config.yaml\", 'r') as ymlfile:\n",
    "        cfg = yaml.load(ymlfile)\n",
    "    args = {k: v for k, v in cfg.items() if k != 'test_datafile'}\n",
    "    f = FeatureGenerator(**args)\n",
    "    f.transform(cfg['test_datafile'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
